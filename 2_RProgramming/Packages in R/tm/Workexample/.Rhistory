gather(key = key, value = death_rate, -age)
head(VADeaths)
worldcup %>%
select(Position, Time, Shots, Tackles, Saves) %>%
gather(Type, Number, -Position, -Time) %>%
ggplot(aes(x = Time, y = Nmber)) +
geom_point() +
facet_grid(Type ~ Position)
worldcup %>%
select(Position, Time, Shots, Tackles, Saves) %>%
gather(Type, Number, -Position, -Time) %>%
ggplot(aes(x = Time, y = Number)) +
geom_point() +
facet_grid(Type ~ Position)
worldcup %>%
select(Position, Time, Shots, Tackles, Saves) %>%
gather(Type, Number, -Position, -Time)
slice(1:10)
worldcup %>%
select(Position, Time, Shots, Tackles, Saves) %>%
gather(Type, Number, -Position, -Time) %>%
slice(1:10)
wc_table <- worldcup %>%
filter(Team %in% c("Spain", "Netherlands", "Uruguay", "Germany")) %>%
select(Team, Position, Passes) %>%
group_by(Team, Position) %>%
summarize(ave_passes = mean(Passes),
min_passes = min(Passes),
max_passes = max(Passes),
pass_summary = paste0(round(ave_passes), " (",
min_passes, ", ",
max_passes, ")")) %>%
select(Team, Position, pass_summary)
wc_table
wc_table %>%
spread(Position, pass_summary) %>%
kable()
wc_table %>%
spread(Position, pass_summary) %>%
table()
library(knitr)
wc_table %>%
spread(Position, pass_summary) %>%
kable()
wc_table
skip()
submit()
names(titanic)
submit()
submit()
submit()
vignette(dplyr)
vignette("dplyr")
vignette("tidyr")
skip()
skip()
paste("Square", "Circle", "Triangle")
paste("Square", "Circle", "Triangle", sep=+)
paste("Square", "Circle", "Triangle", sep="+")
paste0("Square", "Circle", "Triange")
s <- c("Square", "Circle", "Triangle")
paste(s)
s <- c("Square", "Circle", "Triangle")
paste(s)
paste("my favorites are", s)
paste("my favorites is a", s)
cities <- c("Tokyo", "London")
paste("The best city was", city, "the worst city was", city)
city<- c("Tokyo", "London")
paste("The best city was", city, "the worst city was", city)
paste("The", times," city was", city)
city <- c("Tokyo", "London")
times <- c('best', 'worst')
paste("The", times," city was", city)
paste0(c("Square", "Circle", "Triangle"), collapse=TRUE)
paste0(c("Square", "Circle", "Triangle"), collapse=" ")
regular_expression <- "a"
regular_expression2 <- "u"
string_to_search <- "Maryland"
# Searching
grepl(regular_expression, string_to_search)
grepl(regular_expression2, string_to_search)
state.name
head(state.name)
grepl(".", "Maryland")
grepl("\\w", "abcdefghijklmnopqrstuvwxyz0123456789")
grepl("[aeiou]", "rhythms")
grepl("[^aeiou]", "rhythms")
grepl("[a-i]", "rhythms")
grepl("[1-5]", "welcome2014-party")
start_end_vowel <- "^[AEIOU]{1}.+[aeiou]{1}$"
vowel_state_lgl <- grepl(start_end_vowel, state.name)
head(vowel_state_lgl)
state.name[vowel_state_lgl]
library(stringr)
require(stringr)
state_tbl <- paste(state.name, state.area, state.abb)
head(state_tbl)
str_order(state.name)
str_pad("Thai", width = 8, side = "left", pad = "-")
cat(str_wrap(pasted_states, width = 80))
cat(str_wrap(state.name, width = 80))
cat(str_wrap(state.name, width = 60))
cat(str_wrap(state.name, width = 40))
cat(str_wrap(state.name, width = 20))
cat(str_wrap(pasted_states, width = 20))
pasted_states <- paste(state.name[1:20], collapse = " ")
cat(str_wrap(pasted_states, width = 20))
cat(str_wrap(pasted_states, width = 80))
a_tale <- "It was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness"
word(a_tale, 2)
word(a_tale, end = 3)
word(a_tale, start = 11, end =15)
require(pryr)
if (require(pryr) == FALSE) {install.packages("pryr")}
if (require(pryr) == FALSE) {install.packages("pryr");library(pryr)}
ls()
object.size(worldcup)
object_size(worldcup)
if (require(magrittr) == FALSE){install.packages("magrittr"); library(magrittr)}
?get
ls()[1]
get(ls()[1])
get(ls()[i])
1:length(s())
1:length(ls())
for(num in 1:length(ls())){
get(ls()[num])
}
for(num in 1:length(ls())){
print(get(ls()[num]))
}
for(i in 1:length(ls())){
}
print(get(ls()[i]))
for(i in 1:length(ls())){
print(get(ls()[i]))
}
sapply(ls(), function(x) object_size(get(x))) %>% sort %>%tail(5)
get(ls()[1])
object_size(get(ls()[1]))
names(sapply(ls(), function(x) object_size(get(x))) %>% sort %>%tail(5))
high_mem <- names(sapply(ls(), function(x) object_size(get(x))) %>% sort %>%tail(5))
sapply(rm(), high_mem)
high_mem <- names(sapply(ls(), function(x) object_size(get(x))) %>% sort %>%tail(5))
sapply(rm(), ls()[ls() == high_mem])
sapply(high_mem, rm())
?sapply
mem_change(rm(high_mem[1]))
high_mem[1]
class(high_mem[1])
mem_change(rm(get(high_mem[1]))
mem_change(rm(get(high_mem[1])))
mem_change(rm(get(high_mem[1])))
get(high_mem[1])
class(get(high_mem[1]))
rm(get(high_mem[1]))
rm(ls()[1])
ls()
rm("a_tale")
class(rm("a_tale"))
class("a_tale")
rm(high_mem[2])
rm(as.character(high_mem[2]))
mem_change(rm("worldcup"))
object_size(worldcup)
mem_change(rm(worldcup))
mem_change(rm("worldcup"))
ls()
mem_change(rm("titanic"))
ls()
data(titanic)
data("titanic")
data("Titanic")
object_size(Titanic)
mem_change(rm("Titanic"))
ls()
object_size(integer(0))
str(.Machine)
gc()
swirl()
grepl("[Ii]", c("HAwaii", "Illinois", "Kentucky"))
grepl("[Ii]", c("Hawaii", "Illinois", "Kentucky"))
grep("[Ii]", c("Hawaii", "Illinois", "Kentucky"))
sub("[Ii]", "1", c("Hawaii", "Illinois", "Kentucky"))
gsub("[Ii]", "1", c("Hawaii", "Illinois", "Kentucky"))
two_s <- state.name[grep("ss", state.name)]
two_s
strsplit(two_s, "ss")
str_extract("Camaro z28", "[0-9]+")
str_extract("Camaro Z28", "[0-9]+")
str_order(c("p", "e", 'n', 'g'))
str_pad("Thai", width=8, side='left', pad = '-')
str_to_title(c("CAPS", 'low', 'TitLE'))
str_to_title(c("CAPS", 'low', 'Title'))
str_trim(" trim me ")
word("See Spor run.", 2)
word("See Spot run.", 2)
swirl()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
ko2cfLo2WHQQSZBd
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
install.packages("tm")
install.packages("tm", dependencies = TRUE)
remove.packages("tm")
install.packages("tm", dependencies = TRUE)
library(tm)
install.packages(slam)
install.packages("slam")
update.packages("rmarkdown")
install.packages("rmarkdown")
install.packages('tidyverse')
setwd("D:\Dropbox\Programming\Coursera_DataScience\2_RProgramming\Packages in R\tm\Workexample")
setwd("D:/Dropbox/Programming/Coursera_DataScience/2_RProgramming/Packages in R/tm/Workexample")
getwd()
library(tm)
docs <- Corpus(DirSource("./data/"))
docs
str(docs)
str(docs)[1]
docs[1]
docs$SherlockHolmesFailedProjects.txt
docs
writeLines(as.character(docs[[30]]))
getTransformations()
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[1]]))
writeLines(as.character(docs[[1]]))[1:00]
writeLines(as.character(docs[[1]]))[1:100]
writeLines(as.character(docs[[1]]))[1:10]
writeLines(as.character(docs[[1]]))[1:10]
head(writeLines(as.character(docs[[1]])))
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "`")
docs <- tm_map(docs, toSpace, " -")
writeLines(as.character(docs[[1]]))
docs <- tolower(docs)
writeLines(as.character(docs[[1]]))
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
docs <- Corpus(DirSource("./data/"))
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "`")
docs <- tm_map(docs, toSpace, " -")
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[1]]))
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tm)
setwd("D:/Dropbox/Programming/Coursera_DataScience/2_RProgramming/Packages in R/tm/Workexample")
# Chunk 3
docs <- Corpus(DirSource("./data/"))
# Looking at the strucutre of the corpus object
docs
# Chunk 4
# When looking at a particular document use..
writeLines(as.character(docs[[30]]))
# Chunk 5
# There are a couple of transformations availabe as build in functions in the tm package
getTransformations()
# Chunk 6
# create a function to insert spaces after punctuation, where there is none so far
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
# Chunk 7
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
# Chunk 8
# First we remove all punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[1]]))
# Chunk 9
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "`")
docs <- tm_map(docs, toSpace, " -")
writeLines(as.character(docs[[1]]))
# Chunk 10
# First we transform all words to lower case.
docs <- tm_map(docs, content_transformer(tolower))
# Chunk 11
# Strip digits from the text
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[1]]))
# Chunk 12
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[1]]))
getTransformations()
stopwords("english")
stopwords("deutsch")
stopwords("german")
writeLines(as.character(docs[[1]]))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
glimpse <- function(data){ writeLines(as.character(data[[1]]))}
glimpse(docs)
class(stopwords("english"))
stopwords("english")
clean_string <- function(string){
string <- tm_map(string, removePunctuation)
string <- tm_map(string, content_transformer(tolower))
string <- tm_map(string, removeNumbers)
string <- tm_map(string, stripWhitespace)
string <- tm_map(string, stemDocument)
return(string)
}
clean_string("I search for someone to take care of my dog")
string <- Corpus(string)
clean_string <- function(string){
string <- Corpus(string)
string <- tm_map(string, removePunctuation)
string <- tm_map(string, content_transformer(tolower))
string <- tm_map(string, removeNumbers)
string <- tm_map(string, stripWhitespace)
string <- tm_map(string, stemDocument)
return(string)
}
clean_string("I search for someone to take care of my dog")
removePunctuation("This, is my text.")
clean_string <- function(string){
string <- Corpus(x = string)
string <- removePunctuation(string)
string <- tolower(string)
string <- removeNumbers(string)
string <- stripWhitespace(string)
string <- stemDocument(string)
return(string)
}
clean_string("I search for someone to take care of my dog")
clean_string <- function(string){
string <- removePunctuation(string)
string <- tolower(string)
string <- removeNumbers(string)
string <- stripWhitespace(string)
string <- stemDocument(string)
return(string)
}
clean_string("I search for someone to take care of my dog")
clean_string("I search for, 0989,  someone to take care of my dog")
clean_string <- function(string){
string <- removePunctuation(string)
string <- removeWords(string, stopwords("english"))
string <- tolower(string)
string <- removeNumbers(string)
string <- stripWhitespace(string)
string <- stemDocument(string)
return(string)
}
r
clean_string("I search for, 0989,  someone to take care of my dog")
clean_string("Someone to do webdesign, plan the software reviews and do testing.")
clean_string <- function(string){
string <- removePunctuation(string)
string <- removeWords(string, stopwords("english"))
string <- tolower(string)
string <- removeNumbers(string)
string <- stripWhitespace(string)
string <- stemDocument(string)
return(string)
}
clean_string("I search for, 0989,  someone to take care of my dog")
clean_string("Someone to do webdesign, plan the software reviews and do testing.")
clean_string("I need a contant strategist for my marketing department")
clean_string("I need a contant strategist,  for my marketing De{epartment!")
clean_string("I need a contant strategist,  for my m???&^%$@£@arketing D{epartment!")
clean_string("I need,:*(@!2) a contant strategist,  for my m???&^%$@£@arketing D{epartment!")
clean_string("I need,:*(@!2) a contant strategist,  for my m???&^%$@£@arketing Department!")
clean_string("I need,:*(@!2) a contant strategist,  for my m???&^%$@£@arketing DepartmentLASDFK<!")
a <- clean_string("I need,:*(@!2) a contant strategist,  for my m???&^%$@£@arketing DepartmentLASDFK<!")
for(i in a) { print(i)}
i[1]
words(a)[1]
words(a)
class(a)
words(a)
clean_string("If it would be possible for this beautiful system to hire me a content strategist, then i would really really appreciate it")
stopwords()
stripWhitespace("   sssd   sdfsdf   gg g")
?stopwords
stopwords("SMART")
clean_string("If it would be possible for this beautiful system to hire me a content strategist, then i would really really appreciate it")
library(openNLP)
if (require(openNLP) == FALSE){install.packages("openNLP"); library("openNLP")}
if (require(openNLP) == FALSE){install.packages("openNLP"); library(openNLP)}
library(openNLP)
install.packages("rPython")
install.packages("Rpython")
install.packages("rPython")
library(DBI)
library(openNLP)
if (require(openNLP) == FALSE){install.packages("openNLP")}
library(openNLP)
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)}
str <- "this is a the first sentence."
tagged_str <-  tagPOS(str)
}
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)
}
str <- "this is a the first sentence."
tagged_str <-  tagPOS(str
)
str
tagged_str
select_POS <- function(tagged_str, POS){
pos_sel <- strsplit(unlist(tagged_str[1]),'/NN')
pos_sel <- tail(strsplit(unlist(pos_sel[1])," ")[[1]],1)
}
sen1 <- "A Bird was sitting on the roof, as a small girl happily threw the ball up in the air."
sen2 <- "I felt really lucky to be able to get that promotion. Really man, that was what i have always dreamed of!!!"
sen3 <- "Regarding the economic and humanitarian crisis in the middle east, lots of impressive research result have gone lost in explaining this."
corpus <- c(sen1,sen2,sen3)
corpus[1]
corpus[2]
corpus[3]
select_POS(corpus)
select_POS(corpus)
select_POS <- function(tagged_str, POS){
pos_sel <- strsplit(unlist(tagged_str[1]),'/NN')
pos_sel <- tail(strsplit(unlist(pos_sel[1])," ")[[1]],1)
return pos_sel
}
select_POS <- function(tagged_str, POS){
pos_sel <- strsplit(unlist(tagged_str[1]),'/NN')
pos_sel <- tail(strsplit(unlist(pos_sel[1])," ")[[1]],1)
return pos_sel
}
select_POS <- function(tagged_str, POS){
pos_sel <- strsplit(unlist(tagged_str[1]),'/NN')
pos_sel <- tail(strsplit(unlist(pos_sel[1])," ")[[1]],1)
return(pos_sel)
}
select_POS <- function(tagged_str, POS) {
pos_sel <- strsplit(unlist(tagged_str[1]),'/NN')
pos_sel <- tail(strsplit(unlist(pos_sel[1])," ")[[1]],1)
return(pos_sel)
}
corpus <- c(sen1,sen2,sen3)
select_POS(corpus)
select_POS(corpus[1])
select_POS(sen1)
select_POS(sen2)
select_POS(sen2)
tagPOS(sen2)
corpus <- tagPOS(corpus)
corpus
corpus <- lapply(corpus, tagPOS)
corpus
corpus[1]
