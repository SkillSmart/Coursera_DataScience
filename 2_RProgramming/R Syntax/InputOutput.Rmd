---
title: "R_Programming_ReadingWritingData"
author: "Frank Fichtenmueller"
date: "31 October 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R Data IO

## Table of Contents
###Input functions
Principal Funcition to read data into R

* `read.table`, `read.csv` for reading tabular data
* `readLines` for reading lines of a text file
* `source` for reading in R code files (inverse of dump)
* `dget` for reading in R code files (inverse of dput)
* `load` for reading in saved workspaces
* `unserialize` for reading in single R objects in binary form

There are of course many R packages that have been developed to read in datasets, and you might want 
to resort to them when needed. 


###Output functions
Analogous, there are specific functions for *writing to files*:

* `write.table` for writing tabular data to text files (i.e. CSV)
* `writeLines` for writing character data line-by-line to a file or connection
* `dump` for dumping a ntextual reapresentation fo multiple R objects
* `dput` for outputting a textual representation of an R object
* `save` for saving an arbitrary numbe rof R objects in binary fomart (possibly compressed) to a file
* `serialize` for converting a R object into a binary foramt for outputting to a connection (or file)

#### Using textual and Binary Formats to Storing Data

There are a variety of ways to store data, including structured text files like CSV or tab-delimited. 
Additionally there is a R native, intermediate text format using the `dput()` or `dump()` functions.

The reusulting textual format is editable, and potentially recoverable, and unlike writing out to *csv* or 
a *table*, they preserve the metadata. Just sacrifycing some readability. 

Textual formats can work much better with version control programs, as only text based changes can be meaningfully
tracked, and they can be much longer lived, as corruptions can possibly be inspected and fixed.

These formats are not very space effienct though, and only partially readable. 
A storage file as CSV, and a seperate meta-data file might sometimes be more appropriate.

#### Using `dput()` and `dump()`

We can easily deparse an R object with `dput()` and read it back in with `dget()`
```{r}
x <- data.frame(a=1, b=2)
# To inspect the code, we print to the console
dput(x)

# If we want to save, we just pass a `file` argument
dput(x, file="test.R")

# Now we remove x from the namespace...
rm(x); x
```

To get back an object from a `dput` created file
```{r}
source("test.R")
```

#### Working with Binary Formats for storage

Used mainly for efficiency purposes, or because the data can not conveniently be stored in a textual manner. 
With numeric data, it would also cause loss to transform to textual data. 

Keyfunctions used with Binary Formats:

* `save()` [.rda], used to save individual R objects to file
* `save.image()` [.Rdata], used to save a collection of objects. This saves all objects in the workspace to file.
* `serialize()`, used to convert individual R objects into binary format to send over network or connection. -> Raw hexadecimal vector

```{r}
# Create data.frame
x <- data.frame(a=c(1,2,3,4,5), b=c(6,7,8,9,10))
# Save it to file
save(x,file = "test.rd")
# Remove
rm(x);x
# Reload from file
load("test.rd");x
```

Example implementation of the `save.image()` procedure:
```{r}
# Save the current state of the Global Environment
saved_ls <- ls()
length(saved_ls)

rm(list = ls(all=))
```






###IO Packages in R (Deepdive)

## Input Functions (~ Data Import)
### Reading Data files with `read.table()`

The `read.table()` command is the most commonly used functions for reading in data in R. 
The helpfile is worth reading in its entirety, especially because the function gets used a lot.

**TODO: Add the information from the help file in this document **

The `read.table()` function has a couple of important attributes:

* *file* the name of a file, or a connection handler object
* *header*, lobical indicating if the file has a header line
* *sep*, a string indicating how the columns are seperated
* *colClasses*, a character vector indicating the class of each column in the dataset
* *nrows*, the number of rows in the dataset. By default read.table() reads an entire file. 
* *comment.char*, a character string indicating the comment character. Defaults to '#'
* *skip*, the number of lines to skip before starting to read out content
* *stringsAsFactors*, should character variables be coded as factors? Defaults to `TRUE` because back in the old dayas, if you had data that were stored as strings,
it was because they represented levels of a factor. When working with text variables, set this to FALSE and assign factor levels to the suitable variables manually.


When working with small to medium sized datasets, you do not have to worry about setting any other parameters to the `read.table()` function call. 

```{r, eval=FALSE}
data <- read.table('/data/foo.txt')
```


#### Reading larger Datasets with `read.table()`

With much larger datasets, there are a few things that you can do that will make your life easier and prevent memory runouts.

* Read the help page for `read.table()`,  #### TODO: ADD THIS INFORMATION HERE!!!!
* Make a rough calculation of the memory requirements. If this exceeds the memory on your machine, stop right here.
* Set comment.char = "" if there are no commented lines in your files
* us the `colClasses = "numeric"` argument, this can make read.table() run much faster. For this to work you have to know the classes
of each variable. 

Here is a quick and easy way to get a picture of all variable classes:

```{r, eval=FALSE}
# First load just the head of the table. e.g. the first 100 lines
initial <- read.table('datatable.txt', nrows=100)

# Then apply the class function to all columns and store the result in a vector
classes <- sapply(initial, class)

# Load the complete dataset and "manually" define the class for each column, useing the vector
tabAll <- read.table('datatable.txt', colClasses = classes)
```

In general you want to look out for options to reduce the load on the current RAM while performing the data read.

* How much memory is available on your system? 
* What other applications are in use? Can you close any of them? 
* Are there other users logged into the same system? 
* What operating system are you using? Some operatingsystems can limit the amount of memory a single process can access

#### How to calculate the Memory Requirements for an R Object

Remember, R stores all data in physical memory, which severely limits the amount of data you can read in. Before reading in a new dataset, 
you should first calculate the Memory requirements, as R will not take notice until it has run out and all progress on the task will be lost. 

This short and easy calculation should give you a working approximation:

* Double precission floating numbers are stored in 64bit format, or 8 bytes. 

Given that information, we can calculate the requirements with the following formula:

```{r, eval=FALSE}
# Given a data.table(t) we calculate the amount of necessary MB with
nrows(t) * ncol(t) * 8 / 2^20 # To calculate to MB
```

Take into account the current minimum requirements of all other:

* running programs
* the general amount of unassigned RAM on your system (After the initial Memory needs of your operating system)


### `readLines()` for reading lines of a text file

### `source()` for reading in R code files (inverse of dump)

### `dget()` for reading in R code files (inverse of dput)

### `load()` for reading in saved workspaces

### `unserialize()` for reading in single R objects in binary form



## Output functions

### `write.table` for writing tabular data to text files (i.e. CSV)

### `writeLines` for writing character data line-by-line to a file or connection

### `dump` for dumping a ntextual reapresentation fo multiple R objects

### `dput` for outputting a textual representation of an R object

### `save` for saving an arbitrary numbe rof R objects in binary fomart (possibly compressed) to a file

### `serialize` for converting a R object into a binary foramt for outputting to a connection (or file)

## special IO packages in R

Here we will have a indepth look at all the currently available IO packages for R

* **These are:**
    * ** The readr package **
    * **

### Using the `readr` package

Has been recently developed by Hadley Wickham to deal with reading in large flat files quickly. The package proovides replacements for following functions:

-   `read_csv()`: comma separated (CSV) files
-   `read_tsv()`: tab separated files
-   `read_delim()`: general delimited files
-   `read_fwf()`: fixed width files
-   `read_table()`: tabular files where colums are separated by white-space.
-   `read_log()`: web log files

#### Global attributes

- `option(readr.num_columns)`, set number of columns to be printed when reading in a file


#### File Formats 
The package is applicable to reading the following filetypes:

* csv
* tsv
* fwf

These functions have been implemented in C/C++ Code, and offer an exponential performance increase. 
Adittionally they offer *progress meters* to keep track of the state of the data import.

For the most part, these functions can be used analogous to their counterparts, and they offer warning messages when non-fatal excemptions are met. 
The message comes with annotations to the dataFrame, that is helpfull in debugging the problem. 

Especially the functionality of the `read.csv` function has been dramatically improved by Wickham.

It offers :

* a convenient import metric
* a compact method to specify column types (parallel to his work on the dplyr package)

A typical call will take no arguments:
```{r, eval=FALSE}
library(readr)
teams <- read_csv('data/team_standings.csv')
```

The default behaviour includes:

* opening the CSV and importing by line
* Initial read on the first lines of data to decide on column classes



#### Manually specifying the column classes in readr

Using the `col_types=` argument in the `read_csv()` call, we can manually specify the *col_types*

* The argument accepts a compact representation: `col_types= "cc"` will set column 1 & 2 -> 'character', 'character'
    * `col_logical()` [l] for class 'logical'
    * `col_integer()` [i] for class 'integer'
    * `col_double()` [d] for class 'doubles'
    * `col_character()` [c] for class 'character'
    * `col_date(format = "")` [D], with the locales `date_format`
    * `col_time(format = "")` [t], with the locales `time_format`
    * `col_datetime(format = "")` [T], ISO8601 date times
    * `col_number` [n], numbers cointaining the `grouping mark`
    
**To manually specify other column types:**

* `col_skip()`[_,-], don't import this column
* `col_date(format = "")`, dates with given format
* `col_datetime(format, tz)`, date times with given formate. If timezone is UTC, this is 20x faster than `strptime()`
* `col_time(format = "")`, times. Returned as number of seconds past midnight
* `col_factor(levels, ordered)`, parse a fixed set of known values into a factor


Manually specifying the column classes in the read in call:
```{r, eval=FALSE}
# 1st way : Use the shorthand notation
read_csv("iris.csv", col_types = "dc__d")

# Versus the explicit notation that is way..... longer !
read.csv("iris.csv", col_types = cols(
  Sepal.Width = col_double(),
  Sepal.Length = col_double(),
  Petal.Width = col_double(),
  Petal.Length = col_double(),
  Species = col_factor(c("setosa", "versicolor", "virginica"))
))
```


To **just define certain columns** and leave all other to automatic analysis

```{r, eval=FALSE}
read_csv("iris.csv", col_types = cols(
  Species = col_factor(c("setosa", "versicolor", "virginica")))
)
```


Or **set a default type** for unspecified columns:

```{r, eval=FALSE}
read_csv("iris.csv", col_types = cols(
  Species = col_factor(c("setosa", "versicolor", "virginica")),
  .default = col_double())
  )
```

reading **only specific columns** use `cols_only()`

```{r}
read_csv("iris.csv", col_types = cols_only(
  Species = cols_factor(c("setosa", "versicolor", "virginica"))
))
```

```{r}
data <- read_csv(readr_example("mtcars.csv"))
data

# Every table has a spec attribute
s <- spec(data)
s
```

Manually working trough the import of a dataset
```{r}
# 1. Read in a sample of 1000 lines from the dataSet. (can be altered by changing `guess_max =`)
s <- spec_csv(readr_example("mtcars.csv"))

# 2. Automatically set the default to the most common type
cols_condense(s)

# 3.a If the spec has a default of skip then uses cols_only()
s$default <- col_skip()
s

# 3.b Otherwise set the default to the proper type
s$default <- col_character()
s
```

Additional functions introduced with the package:
```{r}
print(s, n=10) # Takes an argument 'n' to specify the amount of columns to display

```

#### Column (class type specific) parsing functions

Each of the `col_xyz()` functions has a corresponding `parse_xyz()` function. 

* `parse_logical()`
* `parse_integer()`
* `parse_double()`
* `parse_character()`
* `parse_number()`
* 


### Using the `readxl` package

The last addition by Hadley Wickham, brings a new R only version of an Excel import function. 

The package provides the following functions:

* `read_excel()` 
    * *arguments* 
        * `path`
        * `sheet`
        * `col_names`
        * `col_types`
        * `na`
        * `skip`

This function provides a generic read of flat datafiles into R without relying on external code bases. 
It is used to read in:

* xls
* xlsx

Example Code:
```{r, eval=FALSE}
dataset <- system.file("extdatea/datasets.xlsx", package = "readxl")
read_excel(datasets)

# Then open a specific sheet by position or name
read_excel(dataset, 2)
read_excel(dataset, "mtcars")

# If NA is present in the data, give the NA Format 
read_excel(dataset, 2, na = "NA")
```



* `excel_sheets()` 
    * arguments
        * path
        
This function provides an interator over all included sheets in an excel workbook. It returns a convenient list.

Example Code:
```{r, eval=FALSE}
excel_sheets(system.file('extdatea/datasets.xlsx', package = 'readxl'))
```

